[{"category":"Runtime","homepage_url":"https://www.anyscale.com/endpoints","id":"runtime--inference-deployment--anyscale-endpoints","logo_url":"https://ai-infra.fun/logos/6d862380e43c41dab4c693ff75baf4079a31fb696210088dc7c3e68fa343cb8f.svg","name":"Anyscale Endpoints","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/anyscale","description":"Run, Fine Tune and Scale LLMs via production-ready APIs","twitter_url":"https://twitter.com/raydistributed"},{"category":"Runtime","homepage_url":"https://www.arcee.ai/","id":"runtime--finetuning-rlhf--arcee","logo_url":"https://ai-infra.fun/logos/fdf713d62bd34b83039256098778ec02ceff51ba7a14cf6c6029278acd0ff595.svg","name":"arcee","subcategory":"Finetuning & RLHF","crunchbase_url":"https://www.crunchbase.com/organization/arcee-ai","description":"Arcee's proprietary SLM domain adaptative language model system helps you adapt and align your AI to your data, leading to more efficient and accurate training of your own SLMs.","twitter_url":"https://twitter.com/arcee_ai"},{"category":"Runtime","homepage_url":"https://www.banana.dev/","id":"runtime--inference-deployment--banana","logo_url":"https://ai-infra.fun/logos/b1eb21e7d264065737d1f7cf71b31f8219ba8cf9d08553300ecc6374b77f8d4c.svg","name":"BANANA","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/banana-d87d","description":"Inference hosting for AI teams who ship fast and scale faster.","twitter_url":"https://twitter.com/bananadev_"},{"category":"Runtime","homepage_url":"https://bentoml.com/","id":"runtime--inference-deployment--bentoml","logo_url":"https://ai-infra.fun/logos/acd25ed4c1475c97474aa4fc02750103596ed7861bf7a6b8b98769b3b2f681b4.svg","name":"bentoml","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/bentoml","description":"BentoML is the platform for software engineers to build AI products.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/bentoml/bentoml","languages":{"Makefile":2655,"JavaScript":1383,"Jinja":11286,"Shell":49926,"Java":2536,"C++":1747,"Python":2265545,"Dockerfile":7104,"Swift":3700,"Go":890,"HTML":3592,"CSS":1125,"PHP":986,"Rust":3177,"Kotlin":1061,"Starlark":17549},"primary":true}],"twitter_url":"https://twitter.com/bentomlai"},{"category":"Runtime","homepage_url":"https://docs.daocloud.io/en/baize/intro/","id":"runtime--inference-deployment--dce-5-0","logo_url":"https://ai-infra.fun/logos/3c8a2385d2891c24d2d53cede6b609f4f524262fab8da3d85bb68771dbf707f4.svg","name":"DCE 5.0","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/daocloud","description":"Baize is a Cloud Native AI module in DCE 5.0 that offers an integrated AI computing platform. It optimizes GPU performance, schedules computing resources efficiently, and provides simplified AI development frameworks to accelerate AI application deployment across industries.","twitter_url":"https://twitter.com/daocloud_io"},{"category":"Runtime","homepage_url":"https://deepinfra.com/","id":"runtime--inference-deployment--deepinfra","logo_url":"https://ai-infra.fun/logos/6fa30ed571358a0d8a6a7b8f435df6e387b0912e9fa9c9836567e85a568aa855.svg","name":"deepinfra","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/deep-infra","description":"Fast ML Inference, Simple API","twitter_url":"https://twitter.com/DeepInfra"},{"category":"Runtime","homepage_url":"https://fireworks.ai/","id":"runtime--inference-deployment--fireworks","logo_url":"https://ai-infra.fun/logos/aaff921a7678ae064ba63ddd157532188babb69096731c237ef07cf6881bda36.svg","name":"fireworks","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/fireworks-ai","description":"Fireworks.ai is a lightning-fast inference platform that helps you serve your large language models (LLMs).","twitter_url":"https://twitter.com/FireworksAI_HQ"},{"category":"Runtime","homepage_url":"https://huggingface.co/","id":"runtime--inference-deployment--huggingface","logo_url":"https://ai-infra.fun/logos/3613c73f07ccae19118bfe6d2f8cd127183d08cf99468a708e090953e116ed0a.svg","name":"huggingface","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/huggingface","description":"The AI community building the future.","maturity":"opensource","repositories":[{"url":"https://github.com/huggingface","primary":true}],"twitter_url":"https://twitter.com/huggingface"},{"category":"Runtime","homepage_url":"https://kserve.github.io/website/latest/","id":"runtime--inference-deployment--kserve","logo_url":"https://ai-infra.fun/logos/4a0df1b4a727c94810de6c68570bb6b920fffd20fc566f88692783fabd1092f8.svg","name":"kserve","subcategory":"Inference & Deployment","description":"Highly scalable and standards based Model Inference Platform on Kubernetes for Trusted A","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/kserve/kserve","languages":{"Shell":87436,"Procfile":44,"Python":2599763,"Go":1670602,"Dockerfile":28662,"Makefile":20421},"primary":true}]},{"category":"Runtime","homepage_url":"https://www.lamini.ai/","id":"runtime--finetuning-rlhf--lamini","logo_url":"https://ai-infra.fun/logos/1adfe59a3144ffff45ab6c385c9fec07c4e6dd82086955cf6da3ee427b68016f.svg","name":"lamini","subcategory":"Finetuning & RLHF","description":"Get LLMs in production in 2 minutes with Lamini!","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/lamini-ai/lamini","languages":{"Python":90711},"primary":true}],"twitter_url":"https://twitter.com/LaminiAI"},{"category":"Runtime","homepage_url":"https://www.lepton.ai/","id":"runtime--inference-deployment--lepton","logo_url":"https://ai-infra.fun/logos/01c0016012c1da578444047da033d9d0ad75163152bacdaa151094885b1e6651.svg","name":"lepton","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/lepton","description":"Run AI applications efficiently, at scale, and in minutes with a cloud native platform.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/leptonai/leptonai","languages":{"Python":646458,"Shell":29524,"Dockerfile":2579},"primary":true}],"twitter_url":"https://twitter.com/leptonAI"},{"category":"Runtime","homepage_url":"https://modal.com/","id":"runtime--inference-deployment--modal","logo_url":"https://ai-infra.fun/logos/bd7eaeacb825b83d40ac14d039af1fec0839cae8bd9fa3db472a8afcb54b30e8.svg","name":"Modal","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modal-labs","description":"Run generative AI models, large-scale batch jobs, job queues, and much more.","twitter_url":"https://twitter.com/modal_labs"},{"category":"Runtime","homepage_url":"https://www.modular.com/max","id":"runtime--inference-deployment--modular-max","logo_url":"https://ai-infra.fun/logos/3847889e9743906eb4237918a9f5ae03e0619495c560880f184f5955061644ae.svg","name":"Modular Max","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modular-ai","description":"Modular is an AI software developer platform that unifies the development and deployment of AI for everyone.","twitter_url":"https://twitter.com/modular"},{"category":"Runtime","homepage_url":"https://octo.ai/","id":"runtime--inference-deployment--octoai","logo_url":"https://ai-infra.fun/logos/3f1559a1586f486661cf4e4a30ccbfdaaca6344e5e3720ce5f66bfe6acb1a40a.svg","name":"OctoAI","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/octoml","description":"OctoAI delivers production-grade GenAI solutions running on the most efficient compute, empowering builders to launch the next generation of AI applications.","twitter_url":"https://twitter.com/octoaicloud"},{"category":"Runtime","homepage_url":"https://docs.open.modelz.ai/","id":"runtime--inference-deployment--openmodelz","logo_url":"https://ai-infra.fun/logos/afd5e53c9c5a2cafb58cf76e88da60eee3db91a0c024853e77d9377c6bd3a6f4.svg","name":"OpenModelZ","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/tensorchord","description":"OpenModelZ is tool to deploy your models to any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/tensorchord/openmodelz","languages":{"Go":684612,"HTML":7695,"Python":1775,"Dockerfile":663,"Makefile":35361,"Shell":38073},"primary":true}],"twitter_url":"https://twitter.com/tensorchord"},{"category":"Runtime","homepage_url":"https://predibase.com/","id":"runtime--finetuning-rlhf--predibase-ludwig","logo_url":"https://ai-infra.fun/logos/64b6e5b18b41fe70a31ec0ec287883816268cdd3be79145097ec00e7309643bf.svg","name":"Predibase (ludwig)","subcategory":"Finetuning & RLHF","crunchbase_url":"https://www.crunchbase.com/organization/predibase","description":"Predibase is a low-code AI platform built for developers. Train, finetune, and deploy any model, from linear regression to large language models. Predibase is built on top of the open source ML framework Ludwig.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/ludwig-ai/ludwig","languages":{"Dockerfile":5924,"Python":5177182},"primary":true}],"twitter_url":"https://twitter.com/ludwig_ai"},{"category":"Runtime","homepage_url":"https://replicate.com/","id":"runtime--inference-deployment--replicate","logo_url":"https://ai-infra.fun/logos/08538bd62a69cd01abebc1d63505bca38e28f5ef033986cc01308caa6e08e0ba.svg","name":"replicate","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/replicate","description":"Run and fine-tune open-source models. Deploy custom models at scale. All with one line of code.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/replicate/cog","languages":{"Python":217646,"Go":192066},"primary":true}],"twitter_url":"https://twitter.com/replicate"},{"category":"Runtime","homepage_url":"https://www.runpod.io/","id":"runtime--inference-deployment--runpod","logo_url":"https://ai-infra.fun/logos/165afe00a4641f5b9e5a4cfb96339e915dde5cd97d5c2be207304112181894db.svg","name":"RunPod","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/runpod","description":"Globally distributed GPU cloud built for production. Develop, train, and scale AI applications.","twitter_url":"https://twitter.com/runpod_io"},{"category":"Runtime","homepage_url":"https://www.seldon.io/solutions/core-plus","id":"runtime--inference-deployment--seldon-core","logo_url":"https://ai-infra.fun/logos/1f1b70945c7466d8c0c55b5a4be3c2941c6219b5013a1536b6dd3da24c741288.svg","name":"Seldon Core","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/seldon","description":"An MLOps framework to package, deploy, monitor and manage thousands of production machine learning models","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/SeldonIO/seldon-core","languages":{"Dockerfile":29907,"Smarty":410,"CMake":3228,"HTML":2840903,"Mustache":1580,"C++":739644,"Makefile":104711,"Starlark":10281,"Jupyter Notebook":1078452,"JavaScript":97933,"Jinja":1998,"R":16102,"Python":1018566,"Go":904186,"Shell":103638,"Java":63415},"primary":true}],"twitter_url":"https://twitter.com/seldon_io"},{"category":"Runtime","homepage_url":"https://tensormatrix.com/","id":"runtime--finetuning-rlhf--tensormatrix","logo_url":"https://ai-infra.fun/logos/91443261f19ba8d3af79d1cbf235f0097f302c11edc2a7bd5e57775020f617f9.svg","name":"tensormatrix","subcategory":"Finetuning & RLHF","description":"An integrated platform to fine-tune, deploy and manage LLM applications.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/TUDB-Labs/multi-lora-fine-tune","languages":{"HTML":33386,"Python":103115},"primary":true}],"twitter_url":"https://twitter.com/Tensor_matrix"},{"category":"Runtime","homepage_url":"https://www.together.ai/","id":"runtime--inference-deployment--together-ai","logo_url":"https://ai-infra.fun/logos/60528c145cc682e5fd43d4b7f11bb1023112c3c952ba6392cb495e45eae6855e.svg","name":"together.ai","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/together-1a7e","description":"The fastest cloud platform for building and running generative AI.","twitter_url":"https://twitter.com/togethercompute"},{"category":"Runtime","homepage_url":"https://docs.vllm.ai/en/latest/","id":"runtime--inference-deployment--vllm","logo_url":"https://ai-infra.fun/logos/2ef3c5d5b2f3d189dabe42a7ed4570c3a568c5880b540f965b5601917fe81248.svg","name":"vLLM","subcategory":"Inference & Deployment","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","languages":{"C++":34783,"Jinja":1840,"Python":1288836,"Dockerfile":3634,"C":3047,"Shell":10216,"Cuda":304960},"primary":true}]}]