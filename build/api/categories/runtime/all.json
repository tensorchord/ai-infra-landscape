[{"category":"Runtime","homepage_url":"https://www.anyscale.com/endpoints","id":"runtime--inference-deployment--anyscale-endpoints","logo_url":"https://ai-infra.fun/logos/6d862380e43c41dab4c693ff75baf4079a31fb696210088dc7c3e68fa343cb8f.svg","name":"Anyscale Endpoints","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/anyscale","description":"Run, Fine Tune and Scale LLMs via production-ready APIs","twitter_url":"https://twitter.com/raydistributed"},{"category":"Runtime","homepage_url":"https://www.arcee.ai/","id":"runtime--finetuning-rlhf--arcee","logo_url":"https://ai-infra.fun/logos/fdf713d62bd34b83039256098778ec02ceff51ba7a14cf6c6029278acd0ff595.svg","name":"arcee","subcategory":"Finetuning & RLHF","crunchbase_url":"https://www.crunchbase.com/organization/arcee-ai","description":"Arcee's proprietary SLM domain adaptative language model system helps you adapt and align your AI to your data, leading to more efficient and accurate training of your own SLMs.","twitter_url":"https://twitter.com/arcee_ai"},{"category":"Runtime","homepage_url":"https://www.banana.dev/","id":"runtime--inference-deployment--banana","logo_url":"https://ai-infra.fun/logos/b1eb21e7d264065737d1f7cf71b31f8219ba8cf9d08553300ecc6374b77f8d4c.svg","name":"BANANA","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/banana-d87d","description":"Inference hosting for AI teams who ship fast and scale faster.","twitter_url":"https://twitter.com/bananadev_"},{"category":"Runtime","homepage_url":"https://bentoml.com/","id":"runtime--inference-deployment--bentoml","logo_url":"https://ai-infra.fun/logos/acd25ed4c1475c97474aa4fc02750103596ed7861bf7a6b8b98769b3b2f681b4.svg","name":"bentoml","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/bentoml","description":"BentoML is the platform for software engineers to build AI products.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/bentoml/bentoml","languages":{"Kotlin":1061,"HTML":3592,"Jinja":11286,"Swift":3700,"C++":1747,"Go":890,"Python":2265545,"Makefile":2655,"Java":2536,"PHP":986,"CSS":1125,"Starlark":17549,"JavaScript":1383,"Dockerfile":7104,"Rust":3177,"Shell":49926},"primary":true}],"twitter_url":"https://twitter.com/bentomlai"},{"category":"Runtime","homepage_url":"https://docs.daocloud.io/en/baize/intro/","id":"runtime--inference-deployment--dce-5-0","logo_url":"https://ai-infra.fun/logos/3c8a2385d2891c24d2d53cede6b609f4f524262fab8da3d85bb68771dbf707f4.svg","name":"DCE 5.0","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/daocloud","description":"Baize is a Cloud Native AI module in DCE 5.0 that offers an integrated AI computing platform. It optimizes GPU performance, schedules computing resources efficiently, and provides simplified AI development frameworks to accelerate AI application deployment across industries.","twitter_url":"https://twitter.com/daocloud_io"},{"category":"Runtime","homepage_url":"https://deepinfra.com/","id":"runtime--inference-deployment--deepinfra","logo_url":"https://ai-infra.fun/logos/6fa30ed571358a0d8a6a7b8f435df6e387b0912e9fa9c9836567e85a568aa855.svg","name":"deepinfra","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/deep-infra","description":"Fast ML Inference, Simple API","twitter_url":"https://twitter.com/DeepInfra"},{"category":"Runtime","homepage_url":"https://fireworks.ai/","id":"runtime--inference-deployment--fireworks","logo_url":"https://ai-infra.fun/logos/aaff921a7678ae064ba63ddd157532188babb69096731c237ef07cf6881bda36.svg","name":"fireworks","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/fireworks-ai","description":"Fireworks.ai is a lightning-fast inference platform that helps you serve your large language models (LLMs).","twitter_url":"https://twitter.com/FireworksAI_HQ"},{"category":"Runtime","homepage_url":"https://geniusrise.ai","id":"runtime--inference-deployment--geniusrise","logo_url":"https://ai-infra.fun/logos/260c0da9e70cef867137d2ee3e54d12c1d4292b71c0b6903444aea0db8870b86.svg","name":"Geniusrise","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/geniusrise","description":"Run, Fine Tune and Scale text, vision, audio and multi-modal models via CLI on local or kubernetes","twitter_url":"https://twitter.com/genius_rise"},{"category":"Runtime","homepage_url":"https://ggml.ai/","id":"runtime--inference-deployment--ggml","logo_url":"https://ai-infra.fun/logos/973fab67e4859dd2e4d625eb143717b07a2f7b66c161cb12f4a2ed51087ebac3.svg","name":"GGML","subcategory":"Inference & Deployment","description":"LLM inference in pure C/C++","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/ggerganov/ggml","languages":{"Metal":268728,"Cuda":471292,"C":1632028,"CMake":39704,"Zig":33357,"Shell":24415,"Objective-C":159563,"C++":1221507,"Swift":1520},"primary":true}],"twitter_url":"https://twitter.com/ggerganov"},{"category":"Runtime","homepage_url":"https://huggingface.co/","id":"runtime--inference-deployment--huggingface","logo_url":"https://ai-infra.fun/logos/3613c73f07ccae19118bfe6d2f8cd127183d08cf99468a708e090953e116ed0a.svg","name":"huggingface","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/huggingface","description":"The AI community building the future.","maturity":"opensource","repositories":[{"url":"https://github.com/huggingface","primary":true}],"twitter_url":"https://twitter.com/huggingface"},{"category":"Runtime","homepage_url":"https://kserve.github.io/website/latest/","id":"runtime--inference-deployment--kserve","logo_url":"https://ai-infra.fun/logos/4a0df1b4a727c94810de6c68570bb6b920fffd20fc566f88692783fabd1092f8.svg","name":"kserve","subcategory":"Inference & Deployment","description":"Highly scalable and standards based Model Inference Platform on Kubernetes for Trusted A","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/kserve/kserve","languages":{"Python":2599763,"Shell":87436,"Procfile":44,"Makefile":20421,"Go":1670602,"Dockerfile":28662},"primary":true}]},{"category":"Runtime","homepage_url":"https://www.lamini.ai/","id":"runtime--finetuning-rlhf--lamini","logo_url":"https://ai-infra.fun/logos/1adfe59a3144ffff45ab6c385c9fec07c4e6dd82086955cf6da3ee427b68016f.svg","name":"lamini","subcategory":"Finetuning & RLHF","description":"Get LLMs in production in 2 minutes with Lamini!","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/lamini-ai/lamini","languages":{"Python":90711},"primary":true}],"twitter_url":"https://twitter.com/LaminiAI"},{"category":"Runtime","homepage_url":"https://www.lepton.ai/","id":"runtime--inference-deployment--lepton","logo_url":"https://ai-infra.fun/logos/01c0016012c1da578444047da033d9d0ad75163152bacdaa151094885b1e6651.svg","name":"lepton","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/lepton","description":"Run AI applications efficiently, at scale, and in minutes with a cloud native platform.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/leptonai/leptonai","languages":{"Python":646458,"Shell":29524,"Dockerfile":2579},"primary":true}],"twitter_url":"https://twitter.com/leptonAI"},{"category":"Runtime","homepage_url":"https://modal.com/","id":"runtime--inference-deployment--modal","logo_url":"https://ai-infra.fun/logos/bd7eaeacb825b83d40ac14d039af1fec0839cae8bd9fa3db472a8afcb54b30e8.svg","name":"Modal","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modal-labs","description":"Run generative AI models, large-scale batch jobs, job queues, and much more.","twitter_url":"https://twitter.com/modal_labs"},{"category":"Runtime","homepage_url":"https://www.modular.com/max","id":"runtime--inference-deployment--modular-max","logo_url":"https://ai-infra.fun/logos/3847889e9743906eb4237918a9f5ae03e0619495c560880f184f5955061644ae.svg","name":"Modular Max","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modular-ai","description":"Modular is an AI software developer platform that unifies the development and deployment of AI for everyone.","twitter_url":"https://twitter.com/modular"},{"category":"Runtime","homepage_url":"https://www.mystic.ai/","id":"runtime--inference-deployment--mystic-ai","logo_url":"https://ai-infra.fun/logos/69532eb19545f21dce0965e60e27f5270af342b694876a8684476544e1732a19.svg","name":"Mystic AI","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/neuro-ai","description":"Run AI models as a scalable and secure API in your cloud or on our serverless cloud.","twitter_url":"https://twitter.com/mysticdotai"},{"category":"Runtime","homepage_url":"https://octo.ai/","id":"runtime--inference-deployment--octoai","logo_url":"https://ai-infra.fun/logos/3f1559a1586f486661cf4e4a30ccbfdaaca6344e5e3720ce5f66bfe6acb1a40a.svg","name":"OctoAI","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/octoml","description":"OctoAI delivers production-grade GenAI solutions running on the most efficient compute, empowering builders to launch the next generation of AI applications.","twitter_url":"https://twitter.com/octoaicloud"},{"category":"Runtime","homepage_url":"https://docs.open.modelz.ai/","id":"runtime--inference-deployment--openmodelz","logo_url":"https://ai-infra.fun/logos/afd5e53c9c5a2cafb58cf76e88da60eee3db91a0c024853e77d9377c6bd3a6f4.svg","name":"OpenModelZ","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/tensorchord","description":"OpenModelZ is tool to deploy your models to any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/tensorchord/openmodelz","languages":{"HTML":7695,"Python":1775,"Shell":38073,"Go":684612,"Dockerfile":663,"Makefile":35361},"primary":true}],"twitter_url":"https://twitter.com/tensorchord"},{"category":"Runtime","homepage_url":"https://predibase.com/","id":"runtime--finetuning-rlhf--predibase-ludwig","logo_url":"https://ai-infra.fun/logos/64b6e5b18b41fe70a31ec0ec287883816268cdd3be79145097ec00e7309643bf.svg","name":"Predibase (ludwig)","subcategory":"Finetuning & RLHF","crunchbase_url":"https://www.crunchbase.com/organization/predibase","description":"Predibase is a low-code AI platform built for developers. Train, finetune, and deploy any model, from linear regression to large language models. Predibase is built on top of the open source ML framework Ludwig.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/ludwig-ai/ludwig","languages":{"Dockerfile":5924,"Python":5177182},"primary":true}],"twitter_url":"https://twitter.com/ludwig_ai"},{"category":"Runtime","homepage_url":"https://replicate.com/","id":"runtime--inference-deployment--replicate","logo_url":"https://ai-infra.fun/logos/08538bd62a69cd01abebc1d63505bca38e28f5ef033986cc01308caa6e08e0ba.svg","name":"replicate","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/replicate","description":"Run and fine-tune open-source models. Deploy custom models at scale. All with one line of code.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/replicate/cog","languages":{"Python":217646,"Go":192066},"primary":true}],"twitter_url":"https://twitter.com/replicate"},{"category":"Runtime","homepage_url":"https://www.runpod.io/","id":"runtime--inference-deployment--runpod","logo_url":"https://ai-infra.fun/logos/165afe00a4641f5b9e5a4cfb96339e915dde5cd97d5c2be207304112181894db.svg","name":"RunPod","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/runpod","description":"Globally distributed GPU cloud built for production. Develop, train, and scale AI applications.","twitter_url":"https://twitter.com/runpod_io"},{"category":"Runtime","homepage_url":"https://www.seldon.io/solutions/core-plus","id":"runtime--inference-deployment--seldon-core","logo_url":"https://ai-infra.fun/logos/1f1b70945c7466d8c0c55b5a4be3c2941c6219b5013a1536b6dd3da24c741288.svg","name":"Seldon Core","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/seldon","description":"An MLOps framework to package, deploy, monitor and manage thousands of production machine learning models","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/SeldonIO/seldon-core","languages":{"CMake":3228,"Go":904186,"JavaScript":97933,"Starlark":10281,"Shell":103638,"Java":63415,"Mustache":1580,"R":16102,"Jinja":1998,"Jupyter Notebook":1078452,"Makefile":104711,"Smarty":410,"C++":739644,"Python":1018566,"Dockerfile":29907,"HTML":2840903},"primary":true}],"twitter_url":"https://twitter.com/seldon_io"},{"category":"Runtime","homepage_url":"https://tensormatrix.com/","id":"runtime--finetuning-rlhf--tensormatrix","logo_url":"https://ai-infra.fun/logos/91443261f19ba8d3af79d1cbf235f0097f302c11edc2a7bd5e57775020f617f9.svg","name":"tensormatrix","subcategory":"Finetuning & RLHF","description":"An integrated platform to fine-tune, deploy and manage LLM applications.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/TUDB-Labs/multi-lora-fine-tune","languages":{"Python":103115,"HTML":33386},"primary":true}],"twitter_url":"https://twitter.com/Tensor_matrix"},{"category":"Runtime","homepage_url":"https://www.together.ai/","id":"runtime--inference-deployment--together-ai","logo_url":"https://ai-infra.fun/logos/60528c145cc682e5fd43d4b7f11bb1023112c3c952ba6392cb495e45eae6855e.svg","name":"together.ai","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/together-1a7e","description":"The fastest cloud platform for building and running generative AI.","twitter_url":"https://twitter.com/togethercompute"},{"category":"Runtime","homepage_url":"https://ubiops.com/","id":"runtime--inference-deployment--ubiops","logo_url":"https://ai-infra.fun/logos/5df6fccbebf227980eec5c3e7ba40367b9c383aeb77d74f55b7962205daf45e1.svg","name":"UbiOps","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/dutch-analytics","description":"Powerful AI model serving and orchestration with unmatched simplicity, speed and scale. In cloud, on-premise, hybrid or multicloud.","twitter_url":"https://twitter.com/UbiOps_"},{"category":"Runtime","homepage_url":"https://docs.vllm.ai/en/latest/","id":"runtime--inference-deployment--vllm","logo_url":"https://ai-infra.fun/logos/2ef3c5d5b2f3d189dabe42a7ed4570c3a568c5880b540f965b5601917fe81248.svg","name":"vLLM","subcategory":"Inference & Deployment","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","languages":{"Shell":10216,"Jinja":1840,"Cuda":304960,"C++":34783,"C":3047,"Python":1288836,"Dockerfile":3634},"primary":true}]},{"category":"Runtime","homepage_url":"https://xorbits.io/","id":"runtime--inference-deployment--xinference","logo_url":"https://ai-infra.fun/logos/a150ef94f99cd18afe58aec1742a2ff1789e16fcda6309aa41ad6c32d1123b0f.svg","name":"xinference","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/xorbits","description":"Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command. Whether you are a researcher, developer, or data scientist, Xorbits Inference empowers you to unleash the full potential of cutting-edge AI models.","maturity":"opensource","oss":true,"repositories":[{"url":"https://github.com/xorbitsai/inference","languages":{"CSS":473,"JavaScript":152918,"HTML":1722,"Python":1007210,"Dockerfile":897},"primary":true}],"twitter_url":"https://twitter.com/xorbitsio"}]