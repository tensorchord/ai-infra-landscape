[{"category":"Runtime","homepage_url":"https://www.anyscale.com/endpoints","id":"runtime--inference-deployment--anyscale-endpoints","logo_url":"https://ai-infra.fun/logos/6d862380e43c41dab4c693ff75baf4079a31fb696210088dc7c3e68fa343cb8f.svg","name":"Anyscale Endpoints","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/anyscale","description":"Run, Fine Tune and Scale LLMs via production-ready APIs","twitter_url":"https://twitter.com/raydistributed"},{"category":"Runtime","homepage_url":"https://www.banana.dev/","id":"runtime--inference-deployment--banana","logo_url":"https://ai-infra.fun/logos/b1eb21e7d264065737d1f7cf71b31f8219ba8cf9d08553300ecc6374b77f8d4c.svg","name":"BANANA","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/banana-d87d","description":"Inference hosting for AI teams who ship fast and scale faster.","twitter_url":"https://twitter.com/bananadev_"},{"category":"Runtime","homepage_url":"https://bentoml.com/","id":"runtime--inference-deployment--bentoml","logo_url":"https://ai-infra.fun/logos/acd25ed4c1475c97474aa4fc02750103596ed7861bf7a6b8b98769b3b2f681b4.svg","name":"bentoml","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/bentoml","description":"BentoML is the platform for software engineers to build AI products.","maturity":"opensource","repositories":[{"url":"https://github.com/bentoml/bentoml","primary":true}],"twitter_url":"https://twitter.com/bentomlai"},{"category":"Runtime","homepage_url":"https://www.confidentialmind.com/","id":"runtime--inference-deployment--confidentialmind","logo_url":"https://ai-infra.fun/logos/eee681287bf845de45b9232a98d854f312cdc143bad206ffab90233b73c643c7.svg","name":"ConfidentialMind","subcategory":"Inference & Deployment","description":"Stack for integrating generative AI with your most confidential data. Deploy to on-premises, hybrid- or private cloud environments.","twitter_url":"https://twitter.com/ConfiMind"},{"category":"Runtime","homepage_url":"https://docs.daocloud.io/en/baize/intro/","id":"runtime--inference-deployment--dce-5-0","logo_url":"https://ai-infra.fun/logos/3c8a2385d2891c24d2d53cede6b609f4f524262fab8da3d85bb68771dbf707f4.svg","name":"DCE 5.0","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/daocloud","description":"Baize is a Cloud Native AI module in DCE 5.0 that offers an integrated AI computing platform. It optimizes GPU performance, schedules computing resources efficiently, and provides simplified AI development frameworks to accelerate AI application deployment across industries.","twitter_url":"https://twitter.com/daocloud_io"},{"category":"Runtime","homepage_url":"https://deepinfra.com/","id":"runtime--inference-deployment--deepinfra","logo_url":"https://ai-infra.fun/logos/6fa30ed571358a0d8a6a7b8f435df6e387b0912e9fa9c9836567e85a568aa855.svg","name":"deepinfra","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/deep-infra","description":"Fast ML Inference, Simple API","twitter_url":"https://twitter.com/DeepInfra"},{"category":"Runtime","homepage_url":"https://fireworks.ai/","id":"runtime--inference-deployment--fireworks","logo_url":"https://ai-infra.fun/logos/aaff921a7678ae064ba63ddd157532188babb69096731c237ef07cf6881bda36.svg","name":"fireworks","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/fireworks-ai","description":"Fireworks.ai is a lightning-fast inference platform that helps you serve your large language models (LLMs).","twitter_url":"https://twitter.com/FireworksAI_HQ"},{"category":"Runtime","homepage_url":"https://geniusrise.ai","id":"runtime--inference-deployment--geniusrise","logo_url":"https://ai-infra.fun/logos/260c0da9e70cef867137d2ee3e54d12c1d4292b71c0b6903444aea0db8870b86.svg","name":"Geniusrise","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/geniusrise","description":"Run, Fine Tune and Scale text, vision, audio and multi-modal models via CLI on local or kubernetes","twitter_url":"https://twitter.com/genius_rise"},{"category":"Runtime","homepage_url":"https://ggml.ai/","id":"runtime--inference-deployment--ggml","logo_url":"https://ai-infra.fun/logos/973fab67e4859dd2e4d625eb143717b07a2f7b66c161cb12f4a2ed51087ebac3.svg","name":"GGML","subcategory":"Inference & Deployment","description":"LLM inference in pure C/C++","maturity":"opensource","repositories":[{"url":"https://github.com/ggerganov/ggml","primary":true}],"twitter_url":"https://twitter.com/ggerganov"},{"category":"Runtime","homepage_url":"https://huggingface.co/","id":"runtime--inference-deployment--huggingface","logo_url":"https://ai-infra.fun/logos/3613c73f07ccae19118bfe6d2f8cd127183d08cf99468a708e090953e116ed0a.svg","name":"huggingface","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/huggingface","description":"The AI community building the future.","maturity":"opensource","repositories":[{"url":"https://github.com/huggingface","primary":true}],"twitter_url":"https://twitter.com/huggingface"},{"category":"Runtime","homepage_url":"https://www.inferless.com/","id":"runtime--inference-deployment--inferless","logo_url":"https://ai-infra.fun/logos/ddb54c7eab0a13d4f74104dcf54878847e2e6ad6bcb0d9557805b63a09750d10.svg","name":"Inferless","subcategory":"Inference & Deployment","description":"Inferless offers serverless GPU Inference to scale your machine learning inference without any hassle of managing servers, and deploy complicated and custom models with ease.","twitter_url":"https://twitter.com/Inferless_"},{"category":"Runtime","homepage_url":"https://kserve.github.io/website/latest/","id":"runtime--inference-deployment--kserve","logo_url":"https://ai-infra.fun/logos/4a0df1b4a727c94810de6c68570bb6b920fffd20fc566f88692783fabd1092f8.svg","name":"kserve","subcategory":"Inference & Deployment","description":"Highly scalable and standards based Model Inference Platform on Kubernetes for Trusted A","maturity":"opensource","repositories":[{"url":"https://github.com/kserve/kserve","primary":true}]},{"category":"Runtime","homepage_url":"https://www.lepton.ai/","id":"runtime--inference-deployment--lepton","logo_url":"https://ai-infra.fun/logos/01c0016012c1da578444047da033d9d0ad75163152bacdaa151094885b1e6651.svg","name":"lepton","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/lepton","description":"Run AI applications efficiently, at scale, and in minutes with a cloud native platform.","maturity":"opensource","repositories":[{"url":"https://github.com/leptonai/leptonai","primary":true}],"twitter_url":"https://twitter.com/leptonAI"},{"category":"Runtime","homepage_url":"https://modal.com/","id":"runtime--inference-deployment--modal","logo_url":"https://ai-infra.fun/logos/bd7eaeacb825b83d40ac14d039af1fec0839cae8bd9fa3db472a8afcb54b30e8.svg","name":"Modal","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modal-labs","description":"Run generative AI models, large-scale batch jobs, job queues, and much more.","twitter_url":"https://twitter.com/modal_labs"},{"category":"Runtime","homepage_url":"https://www.modular.com/max","id":"runtime--inference-deployment--modular-max","logo_url":"https://ai-infra.fun/logos/3847889e9743906eb4237918a9f5ae03e0619495c560880f184f5955061644ae.svg","name":"Modular Max","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/modular-ai","description":"Modular is an AI software developer platform that unifies the development and deployment of AI for everyone.","twitter_url":"https://twitter.com/modular"},{"category":"Runtime","homepage_url":"https://www.mystic.ai/","id":"runtime--inference-deployment--mystic-ai","logo_url":"https://ai-infra.fun/logos/69532eb19545f21dce0965e60e27f5270af342b694876a8684476544e1732a19.svg","name":"Mystic AI","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/neuro-ai","description":"Run AI models as a scalable and secure API in your cloud or on our serverless cloud.","twitter_url":"https://twitter.com/mysticdotai"},{"category":"Runtime","homepage_url":"https://octo.ai/","id":"runtime--inference-deployment--octoai","logo_url":"https://ai-infra.fun/logos/3f1559a1586f486661cf4e4a30ccbfdaaca6344e5e3720ce5f66bfe6acb1a40a.svg","name":"OctoAI","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/octoml","description":"OctoAI delivers production-grade GenAI solutions running on the most efficient compute, empowering builders to launch the next generation of AI applications.","twitter_url":"https://twitter.com/octoaicloud"},{"category":"Runtime","homepage_url":"https://docs.open.modelz.ai/","id":"runtime--inference-deployment--openmodelz","logo_url":"https://ai-infra.fun/logos/afd5e53c9c5a2cafb58cf76e88da60eee3db91a0c024853e77d9377c6bd3a6f4.svg","name":"OpenModelZ","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/tensorchord","description":"OpenModelZ is tool to deploy your models to any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).","maturity":"opensource","repositories":[{"url":"https://github.com/tensorchord/openmodelz","primary":true}],"twitter_url":"https://twitter.com/tensorchord"},{"category":"Runtime","homepage_url":"https://replicate.com/","id":"runtime--inference-deployment--replicate","logo_url":"https://ai-infra.fun/logos/08538bd62a69cd01abebc1d63505bca38e28f5ef033986cc01308caa6e08e0ba.svg","name":"replicate","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/replicate","description":"Run and fine-tune open-source models. Deploy custom models at scale. All with one line of code.","maturity":"opensource","repositories":[{"url":"https://github.com/replicate/cog","primary":true}],"twitter_url":"https://twitter.com/replicate"},{"category":"Runtime","homepage_url":"https://www.runpod.io/","id":"runtime--inference-deployment--runpod","logo_url":"https://ai-infra.fun/logos/165afe00a4641f5b9e5a4cfb96339e915dde5cd97d5c2be207304112181894db.svg","name":"RunPod","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/runpod","description":"Globally distributed GPU cloud built for production. Develop, train, and scale AI applications.","twitter_url":"https://twitter.com/runpod_io"},{"category":"Runtime","homepage_url":"https://www.seldon.io/solutions/core-plus","id":"runtime--inference-deployment--seldon-core","logo_url":"https://ai-infra.fun/logos/1f1b70945c7466d8c0c55b5a4be3c2941c6219b5013a1536b6dd3da24c741288.svg","name":"Seldon Core","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/seldon","description":"An MLOps framework to package, deploy, monitor and manage thousands of production machine learning models","maturity":"opensource","repositories":[{"url":"https://github.com/SeldonIO/seldon-core","primary":true}],"twitter_url":"https://twitter.com/seldon_io"},{"category":"Runtime","homepage_url":"https://www.together.ai/","id":"runtime--inference-deployment--together-ai","logo_url":"https://ai-infra.fun/logos/60528c145cc682e5fd43d4b7f11bb1023112c3c952ba6392cb495e45eae6855e.svg","name":"together.ai","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/together-1a7e","description":"The fastest cloud platform for building and running generative AI.","twitter_url":"https://twitter.com/togethercompute"},{"category":"Runtime","homepage_url":"https://ubiops.com/","id":"runtime--inference-deployment--ubiops","logo_url":"https://ai-infra.fun/logos/5df6fccbebf227980eec5c3e7ba40367b9c383aeb77d74f55b7962205daf45e1.svg","name":"UbiOps","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/dutch-analytics","description":"Powerful AI model serving and orchestration with unmatched simplicity, speed and scale. In cloud, on-premise, hybrid or multicloud.","twitter_url":"https://twitter.com/UbiOps_"},{"category":"Runtime","homepage_url":"https://docs.vllm.ai/en/latest/","id":"runtime--inference-deployment--vllm","logo_url":"https://ai-infra.fun/logos/2ef3c5d5b2f3d189dabe42a7ed4570c3a568c5880b540f965b5601917fe81248.svg","name":"vLLM","subcategory":"Inference & Deployment","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","maturity":"opensource","repositories":[{"url":"https://github.com/vllm-project/vllm","primary":true}]},{"category":"Runtime","homepage_url":"https://xorbits.io/","id":"runtime--inference-deployment--xinference","logo_url":"https://ai-infra.fun/logos/a150ef94f99cd18afe58aec1742a2ff1789e16fcda6309aa41ad6c32d1123b0f.svg","name":"xinference","subcategory":"Inference & Deployment","crunchbase_url":"https://www.crunchbase.com/organization/xorbits","description":"Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command. Whether you are a researcher, developer, or data scientist, Xorbits Inference empowers you to unleash the full potential of cutting-edge AI models.","maturity":"opensource","repositories":[{"url":"https://github.com/xorbitsai/inference","primary":true}],"twitter_url":"https://twitter.com/xorbitsio"}]